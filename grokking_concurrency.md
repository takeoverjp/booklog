# なっとく！並行処理プログラミング

## 1. 並行処理

- 並行処理は、何を行うかと、それをいつ行うかを切り離す
- 並行処理の難しさの一因は、英知が文書化されていないことによる

## 2. 直列実行と並列実行

- グスタフソンの法則
  - アムダールの法則に対するアンチテーゼ
  - 並列化可能な処理を増やしていけば、高速化がプロセッサの数に比例するようになる
- 並行処理：重複する期間に不特定の順序で開始、実行、完了される複数のタスク
- 並列処理：ハードウェア上で同時に実行される複数のタスク

## 3. コンピュータの仕組み

- マルチコアプロセッサCPUは、クロック周波数が高く、広範な命令セットをサポートしていて、それぞれのコアが独立して命令を実行できるが、その複雑度によりGPUと比較してコア数は少ない(MIMD)
- GPUは、クロック周波数はCPUに比べると低く、命令セットも限られており、全てのコアで同一の命令しか実行できないが、非常に多くのコアを持つことで超並列処理を実行できる(SIMD)

## 4. 並行処理の構成要素

- プロセスはリソース（アドレス空間、ファイル、接続など）のコンテナであり、スレッドはそのコンテナの中で実行される命令シーケンス、と考えることができる

## 5. プロセス間通信

- スレッドプールは、ほとんどの処理アプリケーションに適したデフォルトの選択肢であるが、以下の場合は適さない
  - スレッドの優先順位を制御する必要がある
  - スレッド個数の上限が問題になる
  - スレッドのIDが静的に割り当てられなければならない
  - スレッドを特定のタスクに割り当てなければならない

## 6. マルチタスク

- システムリソースを最適に活用するためには、スケジューラが各タスクがIOバウンドなのかCPUバウンドなのかを区別した上で、IOバウンドのタスクからCPUバウンドのタスクにコンテキストを切り替えなければならない

## 7. 分解

- ビッグデータの世界ではETL(Extract, Transform, Load)のパターンに従ったパイプライン化が良く使われる
- 分解方法には、タスク分解とデータ分解がある。パイプラインはタスク分解したあとスループットをあげる手段のひとつ。

## 8. 並行処理問題の解決：競合状態と同期

- 複数のタスクによって同時に実行される可能性があり、かつ共有リソースにアクセスするコードを、クリティカルセクションと呼ぶ
- 二つのタスクが同時に共有リソースにアクセスし、その順番によって結果が変わる状態のことを競合状態と呼ぶ

## 9. 並行処理問題の解決：デッドロックと飢餓状態

- デッドロック：他のタスクが専有しているリソースを複数のタスクが待っていて、どのタスクも実行を再開できない状態
- ライブロック：複数のタスクがリソースを譲り合った結果、排他ロックに対するリクエストが繰り返し拒否され、タスクは実行されたままだが作業が完了しない状態
- 飢餓状態：貪欲なスレッドがリソースを独占した結果、他のスレッドの作業が完了しない状態
- 並行処理のよくある問題
  - プロデューサ／コンシューマ問題
  - リーダ／ライター問題
  - いずれも、セマフォとミューテックスで効率よく解決することができる

## 10. ノンブロッキングI/O

- 多くのOSは、スレッドが数千を超えるとうまく処理できなくなる
- OSスレッド（特にプロセス）は、限られた数の時間のかかるタスクに適している。なぜなら大量のスレッドを使うと、各スレッドのスタックのメモリ消費や、コンテキスト切り替えのコストがパフォーマンスを低下するから。

## 11. イベントベースの並行処理

- イベントループを使ってイベントが発生するのを待ち、それらを処理するパターンをReactorパターンと呼ぶ

## 12. 非同期通信

- ファイバ、軽量スレッド、グリーンスレッドは、コルーチンの別名

## 13. 並行処理アプリケーションを作成する

- Fosterの方法論：並行処理システムを設計するための、４つのステップからなる設計方法論
  - 分割：タスクを分割し、依存関係を整理して、並行して実行できるタスクを識別する。このとき、並行化がコストに見合うかどうかも検討する。
  - 通信：タスクを実行するために必要な通信を決定し、適切な通信構造とアルゴリズムを定義する
  - 凝集化：タスクと責務を特定の問題領域に分割することで、パフォーマンス要件を維持しつつ実装コストを下げる
  - マッピング：全体の実行時間をできるだけ短くするように、物理的なプロセッサにタスクを割り当てる
